# 分词

这个项目包含两个Python脚本，preprocess.py 和 part.py，用于处理文本文件，特别是中文和英文混合的文本。它们首先对文本进行预处理，然后进行分词处理，最终生成每行一个词的文本文件。

## 文件说明
preprocess.py: 此脚本用于读取原始文本文件，尝试不同的编码以正确读取内容，并进行初步的文本清理和格式化，包括移除无意义的字符和空格处理。

part.py: 此脚本接收preprocess.py处理过的文本，进行分词处理。它能够区分中文和英文文本，并使用相应的分词方法。分词后，它会删除空行或只包含特定符号的行，每行输出一个词。

## 使用方法
确保您的环境中安装了 Python 和所需的库：jieba 和 tqdm。

将您想要处理的文本文件放置在名为“文华图专老教师文章”的文件夹中。

运行preprocess.py。这将创建一个名为“input”的新文件夹，其中包含预处理过的文本文件。

接着运行part.py。这将创建一个名为“out”的新文件夹，其中包含分词后的文本文件。

## 共现频率高的词合并

这个分词结果感觉把一些专有名词切分开了，能不能把共现频率高的词进行合并？

特别是在处理词频统计和主题分析时。这种方法可以帮助保留专有名词或短语的完整性，从而提高分析的准确性和可靠性。实现这一目标的一个方法是使用词共现分析和N-gram模型：

- 计算共现频率：首先，您需要计算不同词组的共现频率。在3-gram的情况下，您会考虑三个词同时出现的频率。

- 选择高频共现词组：基于共现分析，您可以选择频率超过某个阈值的三个词的组合。

- 创建N-gram模型：您可以创建一个N-gram模型，其中N可以是3、4甚至更多，取决于您想要合并的单词数量。
